import twint
import nest_asyncio
import pandas as pd
import re
import nltk
import numpy as np
import string
import json
import ast
from gensim.parsing.preprocessing import remove_stopwords
from nltk.stem import PorterStemmer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

nest_asyncio.apply()

#This is only needed if using Google AutoMl
import sys
import proto
import os
from google.cloud import storage
os.environ["GOOGLE_APPLICATION_CREDENTIALS"]="C:/Users/ensli/webanalytics/GCloud key/apikey.json"

from google.api_core.client_options import ClientOptions
from google.cloud import automl_v1


##Create DICT of search values
df = pd.read_excel('company_list2.xlsx', engine='openpyxl')
keys_dict = dict(zip(df['Name'], df['search_term']))
search_terms = keys_dict.values()

##function to scrape twitter with keyword
def scrape_tweets(since, value):
        print(value)
        c = twint.Config()
        c.Search = value
        c.Since = since
        #c.Until = '2020-07-15 12:41:00'
        c.Store_csv = True
        c.Output = "./" + value + '.csv'
        c.Hide_output = True
        c.Count = True
        c.Stats = True
        #c.Resume = 'resume.txt' #Might need to remove this when scraping a list of keywords
        twint.run.Search(c)
        
searchterms = ["'new website'"]

for term in searchterms:
    scrape_tweets('2019-01-01 12:00:00', term)
    
print('Scraping done')

finaldf = pd.read_csv ('new website.csv')

df2 = pd.read_csv  ('skateboarding.csv')
#df3 = pd.read_csv ('Wayfair OR $W3.csv') 
finaldf = finaldf.append(df2,ignore_index=True)
#dataframe = dataframe.append(df3,ignore_index=True)

finaldf = finaldf[['id','date','tweet','language','hashtags','cashtags','user_id','username','name','likes_count','replies_count','retweets_count']]

#finaldf = finaldf[finaldf.tweet.str.contains("\"new website\"")]

#list_of_words_to_exclude = ["conspiracy", "trafficking", "sex", "Trafficking"]

#finaldf = finaldf[~(finaldf.tweet.str.contains("Trafficking")) | ~(finaldf.tweet.str.contains("trafficking")) | ~(finaldf.tweet.str.contains("conspiracy")) | ~(finaldf.tweet.str.contains("sex"))]

#finaldf = finaldf[~(finaldf.tweet.str.contains("Tony"))]

#delete rows in certain date range
finaldf['date'] = pd.to_datetime(finaldf['date']) #change date column datatype to datetime
finaldf = finaldf[~(finaldf['date'].dt.month == 7)] #Change number to correspond to the month you want to exclude
finaldf.head(5)


#1
def remove_punct(text):
    text = re.sub(r"http\S+", " ", text) #remove urls
    text = re.sub(r'\S+\.com\S+',' ',text) #remove urls
    text = re.sub(r'\@\w+',' ',text) #remove mentions
    text = re.sub(r'\#\w+', ' ',text) #remove hashtags
    text = re.sub(r'amp', '', text) #remove amp word. not sure if this works
    text = re.sub('[^A-Za-z]', ' ', text.lower()) #remove non-alphabets
    text = "".join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', ' ', text)
    return text

finaldf['Tweet_punct'] = finaldf['tweet'].apply(lambda x: remove_punct(x))

#3 stopwords
finaldf['Tweet_nonstop'] = finaldf['Tweet_punct'].apply(lambda x: remove_stopwords(x))

#2 Tokenization
def tokenization(text):
    text = re.split('\W+', text)
    return text

finaldf['Tweet_tokenized'] = finaldf['Tweet_nonstop'].apply(lambda x: tokenization(x.lower()))

#Lemmatize
wn = nltk.WordNetLemmatizer()

def lemmatizer(text):
    text = [wn.lemmatize(word) for word in text]
    return text

finaldf['Tweet_lemmatized'] = finaldf['Tweet_tokenized'].apply(lambda x: lemmatizer(x))

#creating a wordcloud
temp=' '.join(finaldf['Tweet_nonstop'].tolist())
wordcloud = WordCloud(width = 800, height = 500, 
                background_color ='white', 
                min_font_size = 10).generate(temp)
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0) 
plt.show()

#finaldf['date'] = finaldf['date'].apply(lambda x: pd.Timestamp(x).strftime('%Y-%m-%d'))
finaldf['date'] = pd.to_datetime(finaldf['date'],dayfirst=True) #change date column datatype to datetime

#Rolling avg number of tweets per day
data1 = finaldf.sort_values('date').set_index('date')
week_groups_resample = data1.resample('D').count()
ax = week_groups_resample["tweet"].rolling(7).mean().plot(kind='line',figsize=(10,5),label="7 day moving avg")
ax = week_groups_resample["tweet"].rolling(30).mean().plot(kind='line',figsize=(10,5),label="30 day moving avg")
ax.set_title(label="Moving average number of tweets per day");
ax.set_xlabel("Timeperiod");
ax.set_ylabel("Number of tweets per day");
leg = ax.legend();
#ax.set_xticklabels(data1.index.strftime('%Y-%m-%d'), rotation=90) 
#plt.xlabel('Date - Week Starting');


#count the number of likes and display in graph]
finaldf['likes_count'].groupby(finaldf['date'].dt.to_period('W')).sum().plot(kind='line')


####VADER Sentiment analysis####

analyser = SentimentIntensityAnalyzer()
scores=[]
for comment in finaldf['tweet']:
    sentiment_score=0
    try:
        sentiment_score=analyser.polarity_scores(comment)['compound']
    except TypeError:
        sentiment_score=0
    
    scores.append(sentiment_score)
    
finaldf['sentiment score']=scores


finaldf['sentiment score'].groupby(finaldf['date'].dt.to_period('d')).mean().plot(kind='line')



##Getting the average sentiment scores for each day
data2 = finaldf.sort_values('date').set_index('date')
sentimentscore = data2['sentiment score'].resample('D').mean()
sentimentscore = pd.DataFrame(sentimentscore)


#Adding the number of tweets per day to the dataframe
sentimentscore['tweets pd'] = week_groups_resample['tweet']



##Calculate thresholds to trigger new trend alert
sentimentscore['Yearly%change'] = sentimentscore['30DMVA'].pct_change(365)


##Calculating 30 day MVA of number of tweets
sentimentscore['30DMVA'] = sentimentscore['tweets pd'].rolling(30).mean()



df = finaldf["date"]
dates= []
from datetime import datetime
for x in df:
    date = datetime.fromordinal(datetime(1900, 1, 1).toordinal() + x - 2)
    dates.append(date)

finaldf["date2"] = dates

finaldf
